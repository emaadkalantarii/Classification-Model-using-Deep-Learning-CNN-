# Signature Classification: Human vs. GenAI Models

This project develops a deep learning sequence classifier to distinguish between handwritten signatures generated by humans and three different GenAI models (GAN, SDT, VAE). The model, implemented in PyTorch, processes 2D coordinate sequences from CSV files to perform this four-class classification.

## Dataset

The dataset is expected to be structured as follows:

/signatures_dataset_root_folder
|_human/
|  |_ 001g01.csv
|  |_ 001g02.csv
|  |_ ...
|_gan/
|  |_ 001g01.csv
|  |_ 001g02.csv
|  |_ ...
|_sdt/
|  |_ 001g01.csv
|  |_ 001g02.csv
|  |_ ...
|_vae/
|  |_ 001g01.csv
|  |_ 001g02.csv
|  |_ ...

Each `.csv` file contains two columns, typically headed 'x' and 'y' (space-separated), representing the 2D coordinates of a signature sequence.

## Objective

The primary goal is to train a sequence classifier that can accurately distinguish between these four categories of signatures:
* **human** (label 0)
* **gan** (label 1)
* **sdt** (label 2)
* **vae** (label 3)

## Methodology

* **Language:** Python 3
* **Core Library:** PyTorch
* **Model Type:** A Bidirectional Gated Recurrent Unit (GRU) network with 2 layers, followed by a dropout layer and a linear classification head.
* **Preprocessing:**
    * Each signature sequence (X, Y coordinates) is loaded from its CSV file.
    * Coordinates are normalized per-signature using Min-Max scaling.
    * Sequences are padded with zeros or truncated to a fixed `MAX_SEQ_LENGTH`.

## Files in the Repository

* `train.py`: Script for training the signature classification model. It handles data loading, preprocessing, model definition, training, validation, and saving the best model checkpoint.
* `eval.py`: Script for evaluating a trained model (`.pth` file) on a new dataset. It loads the model, preprocesses the input data, and predicts labels.
* `requirements.txt`: installing the necessary packages for this project (You can test it yourself by running `pip install -r requirements.txt` in a new virtual environment.)
* `README.md`: This file.

## Setup & Installation

1.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv signature_env
    source signature_env/bin/activate  # On Windows: signature_env\Scripts\activate
    ```

2.  **Install Dependencies:**

    To install the required packages and libraries:
    ```bash
    pip install -r requirements.txt
    ```
    Or install individually:
    ```bash
    pip install torch numpy scikit-learn pandas matplotlib
    ```

## Usage

### 1. Training the Model
Ensure your dataset is in a directory (e.g., ./signatures) with the structure described above. Modify BASE_DATA_DIR in train.py if your data is elsewhere.

python train.py


This will train the model, perform validation, save the best performing model checkpoint as model.pth in the current directory, and finally evaluate it on the test set.

### 2. Evaluating a Trained Model
The eval.py script is designed to be called with a directory containing evaluation data and the path to a trained model file. The load_and_predict function within eval.py will recursively search for .csv files in the provided directory.

python eval.py


When run directly, the if __name__ == "__main__": block in eval.py will attempt to evaluate using ./signatures as the data directory and model.pth. You can modify these paths in the script for local testing. The grading system will call the load_and_predict(directory, model_file) function directly with appropriate arguments.

The load_and_predict function returns a dictionary where keys are absolute file paths of the CSVs and values are the predicted integer labels.

## Hyperparameters

Key hyperparameters are defined at the top of train.py, including:

MAX_SEQ_LENGTH: Maximum sequence length (critically important to tune based on data analysis).
HIDDEN_SIZE: Number of units in the GRU hidden layers (per direction for bidirectional).
NUM_RNN_LAYERS: Number of GRU layers.
DROPOUT_PROB: Dropout rate for regularization.
LEARNING_RATE: Initial learning rate for the Adam optimizer.
WEIGHT_DECAY: L2 regularization strength.
BATCH_SIZE: Number of samples per training batch.
NUM_EPOCHS: Maximum number of training epochs.


## Results
After iterative development, including the use of a 2-layer Bidirectional GRU model and careful hyperparameter tuning (especially MAX_SEQ_LENGTH which should be set based on your dataset analysis - the placeholder 150 in the final code achieved good results but should be verified against your data), the model achieved approximately 80.63% accuracy on the test set. Performance may vary based on the final MAX_SEQ_LENGTH chosen and specific dataset characteristics.
